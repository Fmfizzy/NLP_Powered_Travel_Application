{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==3.1.0\n",
      "  Using cached transformers-3.1.0-py3-none-any.whl.metadata (49 kB)\n",
      "Requirement already satisfied: numpy in j:\\code\\python\\lib\\site-packages (from transformers==3.1.0) (1.22.2)\n",
      "Collecting tokenizers==0.8.1.rc2 (from transformers==3.1.0)\n",
      "  Using cached tokenizers-0.8.1rc2.tar.gz (97 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: packaging in j:\\code\\python\\lib\\site-packages (from transformers==3.1.0) (23.2)\n",
      "Requirement already satisfied: filelock in j:\\code\\python\\lib\\site-packages (from transformers==3.1.0) (3.13.1)\n",
      "Requirement already satisfied: requests in j:\\code\\python\\lib\\site-packages (from transformers==3.1.0) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in j:\\code\\python\\lib\\site-packages (from transformers==3.1.0) (4.65.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in j:\\code\\python\\lib\\site-packages (from transformers==3.1.0) (2023.10.3)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in j:\\code\\python\\lib\\site-packages (from transformers==3.1.0) (0.1.95)\n",
      "Requirement already satisfied: sacremoses in j:\\code\\python\\lib\\site-packages (from transformers==3.1.0) (0.1.1)\n",
      "Requirement already satisfied: colorama in j:\\code\\python\\lib\\site-packages (from tqdm>=4.27->transformers==3.1.0) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in j:\\code\\python\\lib\\site-packages (from requests->transformers==3.1.0) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in j:\\code\\python\\lib\\site-packages (from requests->transformers==3.1.0) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in j:\\code\\python\\lib\\site-packages (from requests->transformers==3.1.0) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in j:\\code\\python\\lib\\site-packages (from requests->transformers==3.1.0) (2021.10.8)\n",
      "Requirement already satisfied: click in j:\\code\\python\\lib\\site-packages (from sacremoses->transformers==3.1.0) (8.1.7)\n",
      "Requirement already satisfied: joblib in j:\\code\\python\\lib\\site-packages (from sacremoses->transformers==3.1.0) (1.3.2)\n",
      "Using cached transformers-3.1.0-py3-none-any.whl (884 kB)\n",
      "Building wheels for collected packages: tokenizers\n",
      "  Building wheel for tokenizers (pyproject.toml): started\n",
      "  Building wheel for tokenizers (pyproject.toml): finished with status 'error'\n",
      "Failed to build tokenizers\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (j:\\code\\python\\lib\\site-packages)\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for tokenizers (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [48 lines of output]\n",
      "      C:\\Users\\Prestige\\AppData\\Local\\Temp\\pip-build-env-nuzlp_y2\\overlay\\Lib\\site-packages\\setuptools\\dist.py:318: InformationOnly: Normalizing '0.8.1.rc2' to '0.8.1rc2'\n",
      "        self.metadata.version = self._normalize_version(self.metadata.version)\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-39\n",
      "      creating build\\lib.win-amd64-cpython-39\\tokenizers\n",
      "      copying tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\n",
      "      creating build\\lib.win-amd64-cpython-39\\tokenizers\\models\n",
      "      copying tokenizers\\models\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\models\n",
      "      creating build\\lib.win-amd64-cpython-39\\tokenizers\\decoders\n",
      "      copying tokenizers\\decoders\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\decoders\n",
      "      creating build\\lib.win-amd64-cpython-39\\tokenizers\\normalizers\n",
      "      copying tokenizers\\normalizers\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\normalizers\n",
      "      creating build\\lib.win-amd64-cpython-39\\tokenizers\\pre_tokenizers\n",
      "      copying tokenizers\\pre_tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\pre_tokenizers\n",
      "      creating build\\lib.win-amd64-cpython-39\\tokenizers\\processors\n",
      "      copying tokenizers\\processors\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\processors\n",
      "      creating build\\lib.win-amd64-cpython-39\\tokenizers\\trainers\n",
      "      copying tokenizers\\trainers\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\trainers\n",
      "      creating build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\base_tokenizer.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\bert_wordpiece.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\byte_level_bpe.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\char_level_bpe.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\sentencepiece_bpe.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "      copying tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\n",
      "      copying tokenizers\\models\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\models\n",
      "      copying tokenizers\\decoders\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\decoders\n",
      "      copying tokenizers\\normalizers\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\normalizers\n",
      "      copying tokenizers\\pre_tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\pre_tokenizers\n",
      "      copying tokenizers\\processors\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\processors\n",
      "      copying tokenizers\\trainers\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\trainers\n",
      "      running build_ext\n",
      "      running build_rust\n",
      "      error: can't find Rust compiler\n",
      "      \n",
      "      If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "      \n",
      "      To update pip, run:\n",
      "      \n",
      "          pip install --upgrade pip\n",
      "      \n",
      "      and then retry package installation.\n",
      "      \n",
      "      If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for tokenizers\n",
      "ERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in j:\\code\\python\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: filelock in j:\\code\\python\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in j:\\code\\python\\lib\\site-packages (from datasets) (1.22.2)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in j:\\code\\python\\lib\\site-packages (from datasets) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in j:\\code\\python\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in j:\\code\\python\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in j:\\code\\python\\lib\\site-packages (from datasets) (1.4.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in j:\\code\\python\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in j:\\code\\python\\lib\\site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: xxhash in j:\\code\\python\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in j:\\code\\python\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in j:\\code\\python\\lib\\site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2023.12.2)\n",
      "Requirement already satisfied: aiohttp in j:\\code\\python\\lib\\site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in j:\\code\\python\\lib\\site-packages (from datasets) (0.20.2)\n",
      "Requirement already satisfied: packaging in j:\\code\\python\\lib\\site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in j:\\code\\python\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in j:\\code\\python\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in j:\\code\\python\\lib\\site-packages (from aiohttp->datasets) (2.0.12)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in j:\\code\\python\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in j:\\code\\python\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in j:\\code\\python\\lib\\site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in j:\\code\\python\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in j:\\code\\python\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in j:\\code\\python\\lib\\site-packages (from huggingface-hub>=0.19.4->datasets) (4.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in j:\\code\\python\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in j:\\code\\python\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in j:\\code\\python\\lib\\site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: colorama in j:\\code\\python\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in j:\\code\\python\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in j:\\code\\python\\lib\\site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in j:\\code\\python\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (j:\\code\\python\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (j:\\code\\python\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers==3.1.0\n",
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 1853 examples [00:00, 1929.10 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('csv', data_files='activity_prompts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['label', 'prompt']\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "j:\\Code\\Python\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(\"J:\\IIT Folder\\Year_4\\FYP\\Code\\Tests/fine_tuned_deberta_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"J:\\IIT Folder\\Year_4\\FYP\\Code\\Tests/fine_tuned_deberta_tokenizer\")\n",
    "new_classifier = pipeline(\"zero-shot-classification\", model=loaded_model,tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'd like recommendations for physical activities that improve coordination.\", 'What are some physical activities good for increasing muscular strength?', 'I need ideas for physical activities that target different muscle groups.', 'Please give me recommendations for physical activities that promote weight management.', 'I want suggestions for physical activities that enhance overall wellness.', 'Could you recommend some physical activities suitable for different age ranges?', \"I'm looking for ideas on physical activities that can be done with minimal space.\", 'What are some enjoyable physical activities for social settings?', 'I need recommendations for physical activities that improve bone density.', 'Please suggest some physical activities that can be done solo.']\n"
     ]
    }
   ],
   "source": [
    "test_set = dataset['train'][90:100]\n",
    "\n",
    "labels = test_set['label']\n",
    "prompts = test_set['prompt']\n",
    "print(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_label is :  budget\n",
      "actual_label is :  physical\n",
      "predicted_label is :  budget\n",
      "actual_label is :  physical\n",
      "predicted_label is :  budget\n",
      "actual_label is :  physical\n",
      "predicted_label is :  budget\n",
      "actual_label is :  physical\n",
      "predicted_label is :  budget\n",
      "actual_label is :  physical\n",
      "predicted_label is :  budget\n",
      "actual_label is :  physical\n",
      "predicted_label is :  budget\n",
      "actual_label is :  physical\n",
      "predicted_label is :  budget\n",
      "actual_label is :  physical\n",
      "predicted_label is :  budget\n",
      "actual_label is :  physical\n",
      "predicted_label is :  budget\n",
      "actual_label is :  physical\n",
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    prompt = prompts[i]\n",
    "    actual_label = labels[i]\n",
    "    result = new_classifier(prompt, candidate_labels=[\"indoor\", \"outdoor\", \"physical\", \"non_physical\", \"engaging\", \"water\", \"budget\", \"luxury\", \"nature\", \"relaxing\"])\n",
    "    predicted_label = result['labels'][0]\n",
    "    print(\"predicted_label is : \", predicted_label)\n",
    "    print(\"actual_label is : \", actual_label)\n",
    "    if predicted_label == actual_label:\n",
    "        correct_predictions += 1\n",
    "    total_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
